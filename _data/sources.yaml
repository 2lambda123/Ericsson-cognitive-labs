- id: doi:10.48550/arXiv.2311.08118
  date: '2023-11-14'
  type: paper
  image: images/light.jpg
  authors:
  - Oscar Llorente Gonzalez
  - Rana Fawzy
  - Jared keown
  - Michal Horemuz
  - "P\xE9ter Vaderna"
  - "S\xE1ndor Laki"
  - "Roland Kotrocz\xF3"
  - Rita Csoma
  - "J\xE1nos M\xE1rk Szalai-Gindl"
  description: Explainability in Graph Neural Networks (GNNs) is a new field growing in the last few years. In this publication we address the problem of determining how important is each neighbor for the GNN when classifying a node and how to measure the performance for this specific task. To do this, various known explainability methods are reformulated to get the neighbor importance and four new metrics are presented. Our results show that there is almost no difference between the explanations provided by gradient-based techniques in the GNN domain. In addition, many explainability techniques failed to identify important neighbors when GNNs without self-loops are used.
  tags:
  - GAI Lab
  - Ericsson GAIA
  - Ericsson Research
  buttons:
  - type: paper
    text: Manuscript
    link: https://arxiv.org/abs/2311.08118
  - type: github
    text: Source Code
    link: EricssonResearch/gnn-neighbors-xai

- id: doi:10.48550/arXiv.2310.19573
  date: '2023-10-30'
  type: paper
  image: images/night.jpg
  authors:
  - Sharath M Shankaranarayana
  description: Supervised machine learning relies on the availability of good labelled data for model training. Labelled data is acquired by human annotation, which is a cumbersome and costly process, often requiring subject matter experts. Active learning is a sub-field of machine learning which helps in obtaining the labelled data efficiently by selecting the most valuable data instances for model training and querying the labels only for those instances from the human annotator. Recently, a lot of research has been done in the field of active learning, especially for deep neural network based models. Although deep learning shines when dealing with image\textual\multimodal data, gradient boosting methods...
  buttons:
  - type: paper
    text: Manuscript
    link: https://arxiv.org/abs/2310.19573


- id: doi:10.48550/arXiv.2309.12913

  date: '2023-09-22'
  type: paper
  image: images/space.jpg
  authors:
  - Oscar Llorente Gonzalez
  - Jaime Boal
  - Eugenio F. Sánchez-Úbeda
  description: ESaliency maps have become one of the most widely used interpretability techniques for convolutional neural networks (CNN) due to their simplicity and the quality of the insights they provide. However, there are still some doubts about whether these insights are a trustworthy representation of what CNNs use to come up with their predictions. This paper explores how rescuing the sign of the gradients from the saliency map can lead to a deeper understanding of multi-class classification problems. Using both pretrained and trained from scratch CNNs we unveil that considering the sign and the effect not only of the correct class, but also the influence of the other classes, allows to better identify the pixels of the image that the network is really focusing on. Furthermore, how occluding or altering those pixels is expected to affect the outcome also becomes clearer.
  tags:
  - GAI Lab
  - Comillas Pontifical University (ICAI)
  buttons:
  - type: paper
    text: Manuscript
    link: https://arxiv.org/abs/2309.12913
  - type: github
    text: Source Code
    link: osllogon/positive_active_saliency_maps
